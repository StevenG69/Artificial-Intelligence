{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245dde7f-b4d6-4032-8293-1e1b4fa182b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class World(object):\n",
    "\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.R = np.zeros(self.x*self.y)\n",
    "        self.agentPos = 0\n",
    "    \n",
    "    # define two functions, idx2xy(self,idx) that gets one index named idx (cell number), and turn it to (x, y) location (for example idx = 5 → (x, y) = (1, 1))\n",
    "    # and a function named xy2idx(self,x,y), that gets (x, y) and turn it to cell number idx (for example (x, y) = (1, 1) → idx = 5)\n",
    "\n",
    "    def idx2xy(self,idx):\n",
    "        ...\n",
    "        return x, y\n",
    "\n",
    "    def xy2idx(self,x,y):\n",
    "        ....\n",
    "        return idx\n",
    "\n",
    "    \n",
    "\n",
    "    def resetAgent(self, pos):\n",
    "        self.agentPos = int(pos)\n",
    "\n",
    "    def setReward(self, x, y, r):\n",
    "        goalState = self.xy2idx(x, y)\n",
    "        self.R[goalState] = r\n",
    "\n",
    "    def getState(self):\n",
    "        return self.agentPos\n",
    "\n",
    "    def getReward(self):\n",
    "        return self.R[self.agentPos]\n",
    "\n",
    "    def getNumOfStates(self):\n",
    "        return self.x*self.y\n",
    " \n",
    "    def getNumOfActions(self):\n",
    "        return 4\n",
    "\n",
    "    def validMove(self,x,y):\n",
    "        valid = True\n",
    "        if x < 0 or x >= self.x:\n",
    "            valid = False\n",
    "        if y < 0 or y >= self.y:\n",
    "            valid = False\n",
    "        return valid\n",
    "\n",
    "    #define move(self,id) that gets id as selected move (a number between 0 to 3) and modify the position of the agent based on the selected move.\n",
    "    # Also, it should update the agent's location only if it is a valid move.\n",
    "    \n",
    "    def move(self,id):\n",
    "        ....\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9218956d-1597-4fdf-8154-b1dc134d3c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, world):\n",
    "        self.world = world\n",
    "        self.numOfActions = self.world.getNumOfActions()\n",
    "        self.numOfStates = self.world.getNumOfStates()\n",
    "        self.Q = np.random.uniform(0.0,0.01,(self.numOfStates,self.numOfActions))\n",
    "        self.alpha = 0.7\n",
    "        self.gamma = 0.4\n",
    "        self.epsilon = 0.25\n",
    "\n",
    "    # epsilon-greedy action selection\n",
    "    # define actionSelection(self, state) as if the random number < epsilon, pick a random action, otherwise pick the max self.Q\n",
    "    def actionSelection(self, state):\n",
    "        ....\n",
    "        return action\n",
    "\n",
    "    # SARSA algorithm that should be completed\n",
    "    def train(self, iter):\n",
    "        for itr in range(iter):\n",
    "\n",
    "            state = int(np.random.randint(0,self.numOfStates))\n",
    "            self.world.resetAgent(state)\n",
    "\n",
    "            # choose action\n",
    "            a = self.actionSelection(state)\n",
    "            expisode = True\n",
    "          \n",
    "            while expisode:\n",
    "                .....\n",
    "                # 1. perform action\n",
    "                # 2. look for reward\n",
    "                # 3. find the state_new \n",
    "                # 4. find the new action\n",
    "                # 5. update Q-values\n",
    "                .....\n",
    "        \n",
    "                state = state_new\n",
    "                a = a_new\n",
    "                \n",
    "                if reward == 1.0:\n",
    "                    self.Q[state_new,:] = 0\n",
    "                    expisode = False\n",
    "\n",
    "        print(self.Q)\n",
    "\n",
    "    def plotQValues(self):\n",
    "        plt.rcParams.update({'font.size': 11})\n",
    "        plt.imshow(self.Q, cmap='Oranges', interpolation='nearest', aspect='auto')\n",
    "        plt.colorbar()\n",
    "        plt.title(\"Q-values\")\n",
    "        plt.xlabel(\"Actions\")\n",
    "        plt.ylabel(\"States\")\n",
    "        plt.xticks(np.arange(4), ('Down', 'Up', 'Right', 'Left'))\n",
    "        plt.yticks(np.arange(self.numOfStates), np.arange(self.numOfStates))\n",
    "        plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301f1b5c-b485-477b-82a7-4a44a8c94b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    world = World(3,4)\n",
    "    world.setReward(2, 3, 1.0) #Goal state\n",
    "    world.setReward(1, 1, -1.0) #Fear region\n",
    "  \n",
    "    learner = Agent(world)\n",
    "    learner.train(1000)\n",
    "    \n",
    "    learner.plotQValues()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
